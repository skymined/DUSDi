"""
Particle í™˜ê²½ì˜ ì‹¤ì œ ë Œë”ë§ ì˜ìƒì„ GIFë¡œ ìƒì„±
Agentë“¤ì´ 2D ê³µê°„ì—ì„œ ì›€ì§ì´ëŠ” ëª¨ìŠµì„ ì‹œê°í™”
"""
import warnings
warnings.filterwarnings('ignore')

import os
os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'
os.environ['MUJOCO_GL'] = 'egl'

import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm

# DUSDi imports
from env_helper import get_single_gym_env
from utils import make_agent, set_seed_everywhere
import utils
from omegaconf import OmegaConf

# ======================
# ì„¤ì •
# ======================
DOMAIN = "particle"
NUM_STEPS = 100  # í”„ë ˆì„ ìˆ˜
SNAPSHOT_TS = 3000000
SEED = 2

# Particle í™˜ê²½ ì„¤ì •
NUM_AGENTS = 10
SKILL_DIM = 5

# í…ŒìŠ¤íŠ¸í•  skill ê°œìˆ˜ ì œí•œ (50ê°œ ì „ì²´ëŠ” ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
MAX_SKILLS_TO_TEST = 50  # ëª¨ë“  agent Ã— ëª¨ë“  skill (10 Ã— 5 = 50ê°œ)

# ëª¨ë¸ ê²½ë¡œ
MODEL_DIR = Path("/home/sky/ë¬¸ì„œ/Github/DUSDi/models/states/particle/seed:2 particle dusdi_diayn test/2")
ACTOR_PATH = MODEL_DIR / f"actor_{SNAPSHOT_TS}.pt"

# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUT_DIR = Path("skill_videos_rendered")
OUT_DIR.mkdir(exist_ok=True)

print("=" * 80)
print("Particle Skill Video Generator (Rendered)")
print("=" * 80)
print(f"Total possible: {NUM_AGENTS * SKILL_DIM} skills")
print(f"Testing: {MAX_SKILLS_TO_TEST} skills")
print(f"Steps per video: {NUM_STEPS}")
print(f"Output: {OUT_DIR}/")
print("=" * 80)

# ======================
# Config ë¡œë“œ
# ======================
from hydra import compose, initialize_config_dir

config_dir = os.path.abspath(".")

with initialize_config_dir(config_dir=config_dir, version_base=None):
    cfg = compose(config_name="pretrain", overrides=[
        f"domain={DOMAIN}",
        f"seed={SEED}",
        "obs_type=states",
        "action_repeat=1",
        "env.particle.N=10",
        "env.particle.simplify_action_space=True",
        "env.particle.use_img=False",  # RGB ë Œë”ë§ ì‚¬ìš©
    ])

set_seed_everywhere(SEED)

# ======================
# í™˜ê²½ ë° Agent ìƒì„±
# ======================
print("\n[Setup] Creating environment with rendering...")
env = get_single_gym_env(cfg, rank=0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from dm_env import specs
obs_spec = specs.Array(shape=env.observation_space.shape, 
                       dtype=env.observation_space.dtype, 
                       name='observation')
action_spec = specs.Array(shape=env.action_space.shape,
                         dtype=env.action_space.dtype,
                         name='action')

agent = make_agent(
    obs_type='states',
    obs_spec=obs_spec,
    action_spec=action_spec,
    num_expl_steps=0,
    parent_cfg=cfg,
    cfg=cfg.agent
)

if ACTOR_PATH.exists():
    with ACTOR_PATH.open('rb') as f:
        actor_state = torch.load(f, map_location=device)
    agent.actor.load_state_dict(actor_state)
    print(f"âœ“ Actor loaded")
else:
    raise FileNotFoundError(f"Actor not found: {ACTOR_PATH}")

# ======================
# ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
# ======================
skill_list = []

# ê° agentë³„ë¡œ ê° skill í…ŒìŠ¤íŠ¸
count = 0
for agent_idx in range(NUM_AGENTS):
    for skill_idx in range(SKILL_DIM):
        if count >= MAX_SKILLS_TO_TEST:
            break
        skill_vec = [0] * 10
        skill_vec[agent_idx] = skill_idx
        skill_name = f"agent{agent_idx}_skill{skill_idx}"
        skill_list.append((skill_vec, skill_name))
        count += 1
    if count >= MAX_SKILLS_TO_TEST:
        break

print(f"\nGenerating {len(skill_list)} video GIFs...")

# ======================
# ê° ìŠ¤í‚¬ ì‹¤í–‰ ë° ë Œë”ë§
# ======================
results = []

for idx, (skill_vec, skill_name) in enumerate(tqdm(skill_list, desc="Rendering videos")):
    # ìŠ¤í‚¬ ì„¤ì •
    skill_vector = np.array(skill_vec, dtype=np.int64)
    meta = agent.get_meta_from_skill(skill_vector, num_envs=1)
    
    # í™˜ê²½ ë¦¬ì…‹
    obs = env.reset()
    if isinstance(obs, tuple):
        obs = obs[0]
    
    # í”„ë ˆì„ ìˆ˜ì§‘
    frames = []
    rewards = []
    
    # ì²« í”„ë ˆì„ ë Œë”ë§
    try:
        frame = env.render(mode='rgb_array')
        if frame is not None:
            frames.append(frame)
    except:
        frame = env.render()
        if frame is not None:
            frames.append(frame)
    
    # ìŠ¤í‚¬ ì‹¤í–‰
    for step in range(NUM_STEPS):
        with torch.no_grad(), utils.eval_mode(agent):
            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)
            action = agent.act(obs_tensor, meta, step, eval_mode=True)
        
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy().flatten()
        else:
            action = action.flatten()
        
        step_result = env.step(action)
        
        if len(step_result) == 5:
            obs, reward, terminated, truncated, info = step_result
            done = terminated or truncated
        else:
            obs, reward, done, info = step_result
        
        rewards.append(reward)
        
        # í”„ë ˆì„ ë Œë”ë§
        try:
            frame = env.render(mode='rgb_array')
            if frame is not None:
                frames.append(frame)
        except:
            frame = env.render()
            if frame is not None:
                frames.append(frame)
        
        if done:
            break
    
    total_reward = sum(rewards)
    
    # ê²°ê³¼ ì €ì¥
    results.append({
        'name': skill_name,
        'skill_vector': skill_vec,
        'reward': total_reward,
        'frames': len(frames)
    })
    
    # ======================
    # GIF ì €ì¥
    # ======================
    if frames:
        # PIL Imageë¡œ ë³€í™˜
        pil_frames = [Image.fromarray(frame) for frame in frames]
        
        # GIF ì €ì¥
        gif_path = OUT_DIR / f"{skill_name}.gif"
        pil_frames[0].save(
            gif_path,
            save_all=True,
            append_images=pil_frames[1:],
            duration=50,  # 50ms per frame = 20 fps
            loop=0  # ë¬´í•œ ë°˜ë³µ
        )
        
        # ë³´ìƒ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        info_path = OUT_DIR / f"{skill_name}_info.txt"
        with open(info_path, 'w') as f:
            f.write(f"Skill: {skill_name}\n")
            f.write(f"Vector: {skill_vec}\n")
            f.write(f"Total Reward: {total_reward:.3f}\n")
            f.write(f"Frames: {len(frames)}\n")

env.close()

# ======================
# ê²°ê³¼ ìš”ì•½
# ======================
print("\n" + "=" * 80)
print("Summary")
print("=" * 80)
print(f"Total GIFs created: {len(results)}")
print(f"Saved to: {OUT_DIR}/")

# ë³´ìƒ ìˆœ ì •ë ¬
results_sorted = sorted(results, key=lambda x: x['reward'], reverse=True)

print("\nTop 10 skills by reward:")
for i, r in enumerate(results_sorted[:min(10, len(results_sorted))]):
    print(f"  {i+1:2d}. {r['name']:20s} | Reward: {r['reward']:7.3f} | Frames: {r['frames']:3d}")

print("\nBottom 10 skills by reward:")
for i, r in enumerate(results_sorted[-min(10, len(results_sorted)):]):
    print(f"  {i+1:2d}. {r['name']:20s} | Reward: {r['reward']:7.3f} | Frames: {r['frames']:3d}")

# CSV ì €ì¥
csv_path = OUT_DIR / "skill_results.csv"
with open(csv_path, 'w') as f:
    f.write("skill_name,skill_vector,reward,frames\n")
    for r in results:
        skill_str = "_".join(map(str, r['skill_vector']))
        f.write(f"{r['name']},{skill_str},{r['reward']:.3f},{r['frames']}\n")

print(f"\nâœ“ Results saved to: {csv_path}")
print("=" * 80)
print(f"\nğŸ¬ All rendered GIF animations saved to: {OUT_DIR}/")
print(f"   View example: eog {OUT_DIR}/agent0_skill1.gif")
print("=" * 80)
print("\nğŸ’¡ To generate all 50 skills, change MAX_SKILLS_TO_TEST=50 in the script")
print("=" * 80)
