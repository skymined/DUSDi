"""
ìŠ¤í‚¬ë³„ Agent ìœ„ì¹˜ ë³€í™”(trajectory)ë¥¼ ë¶„ì„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
Observation ë°ì´í„°ì—ì„œ agent ìœ„ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
"""
import warnings
warnings.filterwarnings('ignore')

import os
os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'
os.environ['MUJOCO_GL'] = 'egl'

import torch
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from tqdm import tqdm

# DUSDi imports
from env_helper import get_single_gym_env
from utils import make_agent, set_seed_everywhere
import utils
from omegaconf import OmegaConf

# ======================
# ì„¤ì •
# ======================
DOMAIN = "particle"
NUM_STEPS = 100
SNAPSHOT_TS = 3000000
SEED = 2

# ë¶„ì„í•  agentì™€ skill
ANALYZE_AGENT = 4  # Agent 0ì˜ trajectory ë¶„ì„
NUM_SKILLS = 5     # 0~4ê¹Œì§€ 5ê°œ skill

MODEL_DIR = Path("/home/sky/ë¬¸ì„œ/Github/DUSDi/models/states/particle/seed:2 particle dusdi_diayn test/2")
ACTOR_PATH = MODEL_DIR / f"actor_{SNAPSHOT_TS}.pt"

print("=" * 80)
print(f"Trajectory Analysis for Agent {ANALYZE_AGENT}")
print("=" * 80)

# ======================
# Config ë¡œë“œ
# ======================
from hydra import compose, initialize_config_dir

config_dir = os.path.abspath(".")

with initialize_config_dir(config_dir=config_dir, version_base=None):
    cfg = compose(config_name="pretrain", overrides=[
        f"domain={DOMAIN}",
        f"seed={SEED}",
        "obs_type=states",
        "action_repeat=1",
        "env.particle.N=10",
        "env.particle.simplify_action_space=True",
    ])

set_seed_everywhere(SEED)

# ======================
# í™˜ê²½ ë° Agent ìƒì„±
# ======================
print("\n[Setup] Creating environment and agent...")
env = get_single_gym_env(cfg, rank=0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from dm_env import specs
obs_spec = specs.Array(shape=env.observation_space.shape, 
                       dtype=env.observation_space.dtype, 
                       name='observation')
action_spec = specs.Array(shape=env.action_space.shape,
                         dtype=env.action_space.dtype,
                         name='action')

agent = make_agent(
    obs_type='states',
    obs_spec=obs_spec,
    action_spec=action_spec,
    num_expl_steps=0,
    parent_cfg=cfg,
    cfg=cfg.agent
)

if ACTOR_PATH.exists():
    with ACTOR_PATH.open('rb') as f:
        actor_state = torch.load(f, map_location=device)
    agent.actor.load_state_dict(actor_state)
    print(f"âœ“ Actor loaded")
else:
    raise FileNotFoundError(f"Actor not found: {ACTOR_PATH}")

# ======================
# ê° ìŠ¤í‚¬ ì‹¤í–‰ ë° trajectory ìˆ˜ì§‘
# ======================
trajectories = {}

print(f"\nCollecting trajectories for Agent {ANALYZE_AGENT}...")

for skill_idx in tqdm(range(NUM_SKILLS), desc="Skills"):
    # Skill vector: ì„ íƒëœ agentë§Œ í•´ë‹¹ skill, ë‚˜ë¨¸ì§€ëŠ” 0
    skill_vec = [0] * 10
    skill_vec[ANALYZE_AGENT] = skill_idx
    skill_vector = np.array(skill_vec, dtype=np.int64)
    
    meta = agent.get_meta_from_skill(skill_vector, num_envs=1)
    
    obs = env.reset()
    if isinstance(obs, tuple):
        obs = obs[0]
    
    # Agent ìœ„ì¹˜ ì €ì¥
    # Observation êµ¬ì¡°: [dist(10), pos_vel(40), extra(20)]
    # Agent iì˜ ìœ„ì¹˜: obs[10 + i*4 : 10 + i*4 + 2] (x, y)
    
    positions = []
    
    for step in range(NUM_STEPS):
        # í˜„ì¬ agent ìœ„ì¹˜ ì¶”ì¶œ
        agent_pos_start = 10 + ANALYZE_AGENT * 4
        agent_x = obs[agent_pos_start]
        agent_y = obs[agent_pos_start + 1]
        positions.append([agent_x, agent_y])
        
        # Action ì„ íƒ ë° ì‹¤í–‰
        with torch.no_grad(), utils.eval_mode(agent):
            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)
            action = agent.act(obs_tensor, meta, step, eval_mode=True)
        
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy().flatten()
        else:
            action = action.flatten()
        
        step_result = env.step(action)
        
        if len(step_result) == 5:
            obs, reward, terminated, truncated, info = step_result
            done = terminated or truncated
        else:
            obs, reward, done, info = step_result
        
        if done:
            break
    
    trajectories[skill_idx] = np.array(positions)

env.close()

# ======================
# Trajectory ë¶„ì„ ë° ì‹œê°í™”
# ======================
print("\n[Analysis] Computing trajectory statistics...")

# 1. Trajectory ê±°ë¦¬ ê³„ì‚° (ì´ ì´ë™ ê±°ë¦¬)
total_distances = {}
for skill_idx, traj in trajectories.items():
    distances = np.sqrt(np.sum(np.diff(traj, axis=0)**2, axis=1))
    total_distances[skill_idx] = np.sum(distances)

# 2. ì‹œì‘ì  ëŒ€ë¹„ ìµœì¢… ìœ„ì¹˜
final_displacements = {}
for skill_idx, traj in trajectories.items():
    displacement = np.linalg.norm(traj[-1] - traj[0])
    final_displacements[skill_idx] = displacement

# 3. Trajectoryê°„ ì°¨ì´ (diversity)
trajectory_diversity = []
for i in range(NUM_SKILLS):
    for j in range(i+1, NUM_SKILLS):
        # ë‘ trajectoryì˜ í‰ê·  ê±°ë¦¬
        traj1 = trajectories[i]
        traj2 = trajectories[j]
        min_len = min(len(traj1), len(traj2))
        diff = np.linalg.norm(traj1[:min_len] - traj2[:min_len], axis=1)
        trajectory_diversity.append(np.mean(diff))

avg_diversity = np.mean(trajectory_diversity)

# ======================
# ì‹œê°í™”
# ======================
fig = plt.figure(figsize=(16, 10))

# 1. Trajectory Plot (2D ê¶¤ì )
ax1 = plt.subplot(2, 3, 1)
colors = ['red', 'blue', 'green', 'orange', 'purple']
for skill_idx, traj in trajectories.items():
    ax1.plot(traj[:, 0], traj[:, 1], 
             color=colors[skill_idx], 
             label=f'Skill {skill_idx}',
             linewidth=2, alpha=0.7)
    ax1.scatter(traj[0, 0], traj[0, 1], color=colors[skill_idx], s=100, marker='o', edgecolors='black')
    ax1.scatter(traj[-1, 0], traj[-1, 1], color=colors[skill_idx], s=100, marker='s', edgecolors='black')
ax1.set_xlabel('X Position')
ax1.set_ylabel('Y Position')
ax1.set_title(f'Agent {ANALYZE_AGENT} Trajectories (â—‹=start, â–¡=end)')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.axis('equal')

# 2. X Position over time
ax2 = plt.subplot(2, 3, 2)
for skill_idx, traj in trajectories.items():
    ax2.plot(traj[:, 0], color=colors[skill_idx], label=f'Skill {skill_idx}', linewidth=2)
ax2.set_xlabel('Time Step')
ax2.set_ylabel('X Position')
ax2.set_title('X Position Over Time')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. Y Position over time
ax3 = plt.subplot(2, 3, 3)
for skill_idx, traj in trajectories.items():
    ax3.plot(traj[:, 1], color=colors[skill_idx], label=f'Skill {skill_idx}', linewidth=2)
ax3.set_xlabel('Time Step')
ax3.set_ylabel('Y Position')
ax3.set_title('Y Position Over Time')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. Total distance traveled
ax4 = plt.subplot(2, 3, 4)
skills = list(total_distances.keys())
distances = [total_distances[s] for s in skills]
ax4.bar(skills, distances, color=colors[:len(skills)])
ax4.set_xlabel('Skill ID')
ax4.set_ylabel('Total Distance Traveled')
ax4.set_title('Movement Distance per Skill')
ax4.grid(True, alpha=0.3)

# 5. Final displacement from start
ax5 = plt.subplot(2, 3, 5)
displacements = [final_displacements[s] for s in skills]
ax5.bar(skills, displacements, color=colors[:len(skills)])
ax5.set_xlabel('Skill ID')
ax5.set_ylabel('Final Displacement')
ax5.set_title('Distance from Starting Point')
ax5.grid(True, alpha=0.3)

# 6. Summary statistics
ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')
summary_text = f"""
Agent {ANALYZE_AGENT} Trajectory Analysis

Average Diversity: {avg_diversity:.4f}
(mean distance between trajectories)

Total Distance Traveled:
"""
for skill_idx in skills:
    summary_text += f"\n  Skill {skill_idx}: {total_distances[skill_idx]:.4f}"

summary_text += f"\n\nFinal Displacement:"
for skill_idx in skills:
    summary_text += f"\n  Skill {skill_idx}: {final_displacements[skill_idx]:.4f}"

ax6.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',
         verticalalignment='center')

plt.tight_layout()
plt.savefig(f'trajectory_analysis_agent{ANALYZE_AGENT}.png', dpi=150)
print(f"\nâœ“ Analysis saved to: trajectory_analysis_agent{ANALYZE_AGENT}.png")

# ======================
# ê²°ë¡ 
# ======================
print("\n" + "=" * 80)
print("Trajectory Analysis Results")
print("=" * 80)
print(f"Average trajectory diversity: {avg_diversity:.4f}")

if avg_diversity < 0.01:
    print("\nâŒ VERY LOW DIVERSITY - Skills produce nearly identical trajectories")
    print("   â†’ Skills are NOT well differentiated")
elif avg_diversity < 0.1:
    print("\nâš ï¸  LOW DIVERSITY - Skills produce similar trajectories")
    print("   â†’ Some differentiation but not strong")
elif avg_diversity < 0.5:
    print("\nâœ“ MODERATE DIVERSITY - Skills produce different trajectories")
    print("   â†’ Skills are reasonably differentiated")
else:
    print("\nâœ“âœ“ HIGH DIVERSITY - Skills produce very different trajectories")
    print("   â†’ Skills are well differentiated!")

print("\nğŸ’¡ Check the plot to see actual trajectory shapes!")
print("=" * 80)
